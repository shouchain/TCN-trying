# -*- coding: utf-8 -*-
"""TCN project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10d8JFJSfYRX98O6DiatYHT2VVOQpeMwe
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tcn import TCN
import os

#os.chdir("/content/drive/My Drive/ElectricDemandForecasting-DL-master/src")

##from models import tcn
#from utils import auxiliary_plots, metrics
#from preprocessing import normalization, data_generation

SEED = 1 
tf.random.set_seed(SEED)
np.random.seed(SEED)

TRAIN_FILE_NAME = '/content/drive/My Drive/ElectricDemandForecasting-DL-master/data/hourly_20140102_20191101_train.csv'
TEST_FILE_NAME = '/content/drive/My Drive/ElectricDemandForecasting-DL-master/data/hourly_20140102_20191101_test.csv'

FORECAST_HORIZON = 24
PAST_HISTORY = 192
BATCH_SIZE = 256
BUFFER_SIZE = 10000
EPOCHS = 25
METRICS = ['mape']

TCN_PARAMS = {
    'nb_filters': 128,
    'kernel_size': 3,
    'nb_stacks': 1,
    'dilations': [1, 2, 4, 8, 16, 32, 64],
    'dropout_rate': 0,
}

# Read train file
with open(TRAIN_FILE_NAME, 'r') as datafile:
    ts_train = datafile.readlines()[1:]  # skip the header 略過開頭
    ts_train = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_train])
    ts_train = np.reshape(ts_train, (ts_train.shape[0],))

    
# Read test data fil
with open(TEST_FILE_NAME, 'r') as datafile:
    ts_test = datafile.readlines()[1:]  # skip the header
    ts_test = np.asarray([np.asarray(l.rstrip().split(',')[0], dtype=np.float32) for l in ts_test])
    ts_test = np.reshape(ts_test, (ts_test.shape[0],))

TRAIN_SPLIT = int(ts_train.shape[0] * 0.8)

# MIN_MAX NORMALIZATION
# Normalize training data
norm_params = normalization.get_normalization_params(ts_train[:TRAIN_SPLIT])
ts_train = normalization.normalize(ts_train, norm_params)

# Normalize test data with train params
ts_test = normalization.normalize(ts_test, norm_params)

# Get x and y for training and validation (把數據切成模型的模樣)
x_train, y_train = data_generation.univariate_data(ts_train, 0, TRAIN_SPLIT, PAST_HISTORY, FORECAST_HORIZON)
x_val, y_val = data_generation.univariate_data(ts_train, TRAIN_SPLIT - PAST_HISTORY, ts_train.shape[0],
                                                   PAST_HISTORY, FORECAST_HORIZON)

# Get x and y for test data
x_test, y_test = data_generation.univariate_data(ts_test, 0, ts_test.shape[0], PAST_HISTORY, FORECAST_HORIZON)

# Convert numpy data to tensorflow dataset
train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE).repeat()
test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)

model = tcn(x_train.shape, FORECAST_HORIZON, 'adam', 'mae', 
            nb_filters=TCN_PARAMS['nb_filters'],
            kernel_size=TCN_PARAMS['kernel_size'],
            nb_stacks= TCN_PARAMS['nb_stacks'],
            dilations=TCN_PARAMS['dilations'],
            dropout_rate=TCN_PARAMS['dropout_rate'])

model.summary()

TRAIN_MODEL = True   

checkpoint_path = "training_tcn/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

if TRAIN_MODEL:
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                     save_weights_only=True,
                                                     verbose=1)
    evaluation_interval = int(np.ceil(x_train.shape[0] / BATCH_SIZE))
    history = model.fit(train_data, 
                        epochs=EPOCHS,
                        steps_per_epoch=evaluation_interval,
                        validation_data=val_data, validation_steps=evaluation_interval,
                        callbacks=[cp_callback])
    auxiliary_plots.plot_training_history(history, ['loss'])

model.load_weights(checkpoint_path)

"""## ITRI DATA SET"""

itri_data = pd.read_csv('/content/drive/My Drive/工研院電表資料/data_1506787200_1509465599(201710)_out.csv')
itri_test = pd.read_csv('/content/drive/My Drive/工研院電表資料/data_1509465600_1512057599(201711)_out.csv')

pd.unique(itri_data['meterID'])

# training set
itri_data = itri_data[itri_data['meterID']=='AD17000015'].reset_index()
itri_test = itri_test[itri_test['meterID']=='AD17000015'].reset_index()
itri_data_train = []
itri_data_test = []
for i in range(len(itri_data)):
  if i == (len(itri_data)-1):
    continue
  else:
    a = itri_data['value'][i+1] - itri_data['value'][i]
    itri_data_train.append(a)

mean = sum(itri_data_train)/len(itri_data_train)
itri_data_train.append(mean)

#testing set
for i in range(len(itri_test)):
  if i == (len(itri_test)-1):
    continue
  else:
    a = itri_test['value'][i+1] - itri_test['value'][i]
    itri_data_test.append(a)

mean = sum(itri_data_test)/len(itri_data_test)
itri_data_test.append(mean)

# training set
train = np.array(itri_data_train, dtype=np.float32 )
train = np.reshape(train, (train.shape[0],))
train_split = int(train.shape[0] * 0.8)
norm_params = normalization.get_normalization_params(train[:train_split])
train = normalization.normalize(train, norm_params)

#testing set
test = np.array(itri_data_test, dtype=np.float32 )
test = np.reshape(test, (test.shape[0],))
test_split = int(test.shape[0] * 0.8)
norm_params = normalization.get_normalization_params(test[:test_split])
test = normalization.normalize(test, norm_params)

x_train, y_train = data_generation.univariate_data(train, 0, train_split, PAST_HISTORY, FORECAST_HORIZON)
x_val, y_val = data_generation.univariate_data(train, train_split - PAST_HISTORY, train.shape[0],
                                                   PAST_HISTORY, FORECAST_HORIZON)

x_test, y_test = data_generation.univariate_data(test, 0, len(test), PAST_HISTORY, FORECAST_HORIZON)

train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE).repeat()
test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).repeat()

model = tcn(x_train.shape, FORECAST_HORIZON, 'adam', 'mape', 
            nb_filters=TCN_PARAMS['nb_filters'],
            kernel_size=TCN_PARAMS['kernel_size'],
            nb_stacks= TCN_PARAMS['nb_stacks'],
            dilations=TCN_PARAMS['dilations'],
            dropout_rate=TCN_PARAMS['dropout_rate'])

model.summary()

TRAIN_MODEL = True   

checkpoint_path = "training_tcn/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

if TRAIN_MODEL:
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                     save_weights_only=True,
                                                     verbose=1)
    evaluation_interval = int(np.ceil(x_train.shape[0] / BATCH_SIZE))
    history = model.fit(train_data, 
                        epochs=EPOCHS,
                        steps_per_epoch=evaluation_interval,
                        validation_data=val_data, validation_steps=evaluation_interval,
                        callbacks=[cp_callback])
    auxiliary_plots.plot_training_history(history, ['loss'])

model.load_weights(checkpoint_path)